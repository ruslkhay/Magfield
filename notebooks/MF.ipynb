{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "nRXvdqOdWwfB"
   },
   "source": [
    "### Jotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "F9Me7HxzW4cn"
   },
   "source": [
    "* [number types](https://numpy.org/devdocs/user/basics.types.html).\n",
    "* [plotly 3D](https://plotly.com/python/3d-charts/)\n",
    "* [plotly documentation](https://plotly.github.io/plotly.py-docs/index.html)\n",
    "* [plotly fig layout](https://plotly.com/python/reference/layout/#layout-title)\n",
    "* [генератор политры цвета](https://coolors.co/092327-0b5351-00a9a5-4e8098-90c2e7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "9co2aKWFXFiC"
   },
   "source": [
    "# Data and modules loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Modules loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from plotly.express import histogram, line\n",
    "\n",
    "import tensorflow_probability\n",
    "from src.em.mixture import Mixture, DynamicMixture\n",
    "\n",
    "\n",
    "# Позволяет использовать измененные модули без перезагрузки ядра\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.read_csv(\"nice_jan_march.csv\")\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = [\n",
    "#     'Year',\n",
    "#     'Day',\n",
    "#     'Hour',\n",
    "#     'Minute',\n",
    "#     'Bx',\n",
    "#     'By',\n",
    "#     'Bz',\n",
    "#     'Vx',\n",
    "#     'Vy',\n",
    "#     'Vz'\n",
    "# ]\n",
    "# dataframe = pd.read_table('Data/magfield23.acs', sep=' +', header=None, names=names)\n",
    "# dataframe.head()\n",
    "# dataframe.to_csv('Data/magfield23.csv', index=False)\n",
    "# from auxiliary import time_related_id\n",
    "# time_related_id(dataframe)\n",
    "dataframe = pd.read_csv(\"Data/magfield23_ydhm.csv\", na_values=[\"99999.9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increm(arr):\n",
    "    \"\"\"\n",
    "    Calculate increments of given array.\n",
    "    First value is NaN by default\n",
    "    \"\"\"\n",
    "    new_ar = [None]\n",
    "    for i in range(1, len(arr)):\n",
    "        inc = arr[i] - arr[i - 1]\n",
    "        new_ar.append(inc)\n",
    "    return new_ar\n",
    "\n",
    "\n",
    "dataframe[\"dBx\"] = increm(dataframe[\"Bx\"].values)\n",
    "dataframe[\"dBy\"] = increm(dataframe[\"By\"].values)\n",
    "dataframe[\"dBz\"] = increm(dataframe[\"Bz\"].values)\n",
    "dataframe[\"dVx\"] = increm(dataframe[\"Vx\"].values)\n",
    "dataframe[\"dVy\"] = increm(dataframe[\"Vy\"].values)\n",
    "dataframe[\"dVz\"] = increm(dataframe[\"Vz\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "742467d0"
   },
   "source": [
    "# Choosing best period for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "PmFTfjjTfOZi"
   },
   "source": [
    "## Visuailzation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "4s4L7ADEfcTf"
   },
   "source": [
    "### Time series and their histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.em.monitor import show_genral_info\n",
    "\n",
    "show_genral_info(\n",
    "    series=data_clean[\"BX\"],\n",
    "    add_title='Пропуски \"склеены\".',\n",
    "    add_xaxis=data_clean[\"ydhm_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "fobxJD6AfkQw"
   },
   "source": [
    "### 3D histograms for time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Here is code for 3D histogram visualization. It is implemented into \n",
    "DynamicMixture class as you will see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist3D(data, window_size, bins, step):\n",
    "    def construct_hist3D(data, window_size, bins, step):\n",
    "        \"\"\"\n",
    "        Функция составляет 3D гистограммы.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        data : pandas.core.series.Series\n",
    "            Колонка таблицы pandas.core.frame.DataFrame.\n",
    "        window_size : int\n",
    "            Длина поднабора (окна) data, который будет использоваться при анализе.\n",
    "        bins : int\n",
    "            Ширина ячейки гистограммы.\n",
    "        step : int\n",
    "            Величина смещения окна, относительно предыдущего.\n",
    "\n",
    "        Возвращает:\n",
    "        ----------\n",
    "        df: pandas.core.frame.DataFrame\n",
    "            Таблица с координатами точек привязок: bins, wind_numb, -\n",
    "            и высотами столбцов - hist_freq.\n",
    "            df.attrs: dict\n",
    "                Содержит вспомогательную информацию, используемую при кастомизации.\n",
    "        \"\"\"\n",
    "        from pandas import DataFrame\n",
    "        from numpy import histogram, meshgrid\n",
    "\n",
    "        num_windows = len(data) - window_size + 1\n",
    "        # file_save_name = f\"{data.name}-l{len(data)}-ws{window_size}-s{step}-b{bins}\"\n",
    "        df = DataFrame({\"bins\": [], \"wind_numb\": [], \"hist_freq\": []})\n",
    "\n",
    "        df.attrs = {\n",
    "            \"data_name\": data.name,\n",
    "            \"data_length\": len(data),\n",
    "            \"window_size\": window_size,\n",
    "            \"step_size\": step,\n",
    "            \"bin_size\": bins,\n",
    "        }\n",
    "\n",
    "        for i in range(0, num_windows, step):\n",
    "            window = data[i : i + window_size]\n",
    "            hist, _bins = histogram(window, bins=bins)\n",
    "            xpos, ypos = meshgrid(_bins[:-1], i)\n",
    "            xpos = xpos.flatten()\n",
    "            ypos = ypos.flatten()\n",
    "            dz = hist.flatten()\n",
    "            df.loc[len(df.index) - 1] = [xpos, ypos, dz]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def visualize_3D_hist(hist3D):\n",
    "        \"\"\"\n",
    "        Строит объемный график, представляющий динамику изменения гистограмм в\n",
    "        зависимости от положения окна. Сечение, перпендикулярное оси y \"№ Окна\" -\n",
    "        это гистограмма в соответсвующем окне.\n",
    "        \"\"\"\n",
    "        import plotly.graph_objects as go\n",
    "\n",
    "        # Выделение данных\n",
    "        x = hist3D[\"bins\"].values\n",
    "        y = hist3D[\"wind_numb\"].values\n",
    "        z = hist3D[\"hist_freq\"].values\n",
    "\n",
    "        # Построение 3D поверхности\n",
    "        fig = go.Figure(data=[go.Surface(z=z, x=x, y=y)])\n",
    "\n",
    "        # Персонализация изолиний и проекции\n",
    "        custom_contours_z = dict(\n",
    "            show=True, usecolormap=True, highlightcolor=\"limegreen\", project_z=True\n",
    "        )\n",
    "        fig.update_traces(contours_z=custom_contours_z)\n",
    "\n",
    "        # Персонализация осей\n",
    "        custom_scene = dict(\n",
    "            xaxis=dict(title=\"Интервалы гист-ы\", color=\"grey\"),\n",
    "            yaxis=dict(title=\"№ Окна\", color=\"grey\"),\n",
    "            zaxis=dict(\n",
    "                title=\"Приращения \" + hist3D.attrs.get(\"data_name\") + \", нТ\",\n",
    "                color=\"grey\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Название графика\n",
    "        custom_title = (\n",
    "            f\"Компонента: {hist3D.attrs.get('data_name')}, \"\n",
    "            f\"кол-во данных: {hist3D.attrs.get('data_length')}, \"\n",
    "            f\"размер окна: {hist3D.attrs.get('window_size')}, \"\n",
    "            f\"кол-во интервалов {hist3D.attrs.get('bin_size')}, \"\n",
    "            f\"длина шага: {hist3D.attrs.get('step_size')}.\"\n",
    "        )\n",
    "\n",
    "        # Персонализация графика\n",
    "        fig.update_layout(\n",
    "            title=custom_title,\n",
    "            scene=custom_scene,\n",
    "            autosize=True,\n",
    "            width=1200,\n",
    "            height=600,\n",
    "            margin=dict(l=65, r=50, b=65, t=90),\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "    return visualize_3D_hist(construct_hist3D(data, window_size, bins, step))\n",
    "\n",
    "\n",
    "hist3D(dataframe[\"Bx\"][:9000], window_size=4300, step=30, bins=40).show()\n",
    "hist3D(dataframe[\"By\"][:9000], window_size=4300, step=30, bins=40).show()\n",
    "hist3D(dataframe[\"Bz\"][:9000], window_size=4300, step=30, bins=40).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "C-ue-9aMAJSa"
   },
   "source": [
    "# ЕМ-algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Desctiption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "* __Е-step__\n",
    "\n",
    "    1. Calculate unnormalized responsibilities: \n",
    "    $$\n",
    "    \\normalsize{ \\quad \\tilde\\rho_k^{[i]} = \\pi_k \\cdot \\frac{1}{\\sigma_k \\sqrt{2\\pi}} \\cdot \\exp{\\left(-\\frac{(x^{[i]} - \\mu_k)^2}{2\\sigma^2}\\right)} \\equiv\n",
    "    \\pi_k \\cdot \\frac{1}{\\sigma_k} \\cdot \\varphi \\left(\\frac{x^{[i]} - \\mu_k}{2\\sigma} \\right) }\n",
    "    $$\n",
    "    2. Normilize responsibilities: \n",
    "    $$\n",
    "    \\normalsize{ \\quad \\rho_k^{[i]} = \\frac{\\tilde\\rho_k^{[i]}}{\\sum_{k=0}^{M-1} \\tilde\\rho_k^{[i]}} }\n",
    "    $$\n",
    "    3. Calculate class responsibilities: \n",
    "    $$\n",
    "    \\normalsize{ \\quad \\gamma_k = \\sum_{i=0}^{N-1} \\rho_k^{[i]} }\n",
    "    $$\n",
    "    \n",
    "* __М-step__\n",
    "\n",
    "    1. Update the class probabilities: \n",
    "    $$\n",
    "    \\normalsize{ \\quad \\pi_k = \\frac{\\quad \\gamma_k}{N} }\n",
    "    $$\n",
    "    2. Update the math. expectations: \n",
    "    $$\n",
    "    \\normalsize{ \\quad \\mu_k = \\frac{1}{\\gamma_k} \\cdot \\sum_{i=0}^{N-1} \\rho_k^{[i]}x^{[i]} }\n",
    "    $$\n",
    "    3. Update the standard deviations:\n",
    "    $$\n",
    "    \\normalsize{ \\quad \\sigma_k = \\sqrt{\\frac{1}{\\gamma_k} \\cdot \\sum_{i=0}^{N-1} \\rho_k^{[i]}\\left(x^{[i]} - \\mu_k \\right)^2} }\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Generate mixture, check EMs on valid initial parameters, compare results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mixture model\n",
    "m = Mixture(num_comps=3, distrib=tensorflow_probability.distributions.Normal)\n",
    "np.random.seed()\n",
    "rseed = np.random.randint(1_000)\n",
    "m.initialize_probs_mus_sigmas(random_seed=rseed)\n",
    "print(f\"radnom seed equal to {rseed}\")\n",
    "# Generate data for testing\n",
    "m.generate_samples(1_000, random_seed=57)\n",
    "# another_data = m.construct_tpf_mixture().sample(1_000).numpy()\n",
    "test_data1 = m.samples\n",
    "# test_data2 = another_data\n",
    "\n",
    "\n",
    "## additional stuff; output appearancece\n",
    "def __round(numb, dig=4):\n",
    "    return round(numb, dig)\n",
    "\n",
    "\n",
    "def __Round(arr, dig=4):\n",
    "    return list(map(lambda numb: round(numb, dig), arr))\n",
    "\n",
    "\n",
    "def custom_print(text, *args, func=__Round):\n",
    "    print(\n",
    "        text,\n",
    "        f\"Orig - {func(args[0])}\",\n",
    "        f\"Iter - {func(args[1])}\",\n",
    "        f\"Adap - {func(args[2])}\",\n",
    "        sep=\"\\n\\t\",\n",
    "    )\n",
    "\n",
    "\n",
    "# EM comparison\n",
    "pit, mit, sit, llhit = m.EM_iterative(test_data1, 30)\n",
    "pad, mad, sad, llhad = m.EM_adaptive(test_data1, 0.001)\n",
    "\n",
    "custom_print(\"Components probabilities:\", m.probs, pit, pad)\n",
    "custom_print(\"Mathematical expectations:\", m.mus, mit, mad)\n",
    "custom_print(\"Standard deviations:\", m.sigmas, sit, sad)\n",
    "custom_print(\n",
    "    \"Log-likelihoods:\", m.log_likelihood(test_data1), llhit, llhad, func=__round\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Checking sieving EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed()\n",
    "rseed = np.random.randint(1_000)\n",
    "m = Mixture(\n",
    "    num_comps=3,\n",
    "    distrib=tensorflow_probability.distributions.Normal,\n",
    "    random_seed=rseed,\n",
    "    rand_initialize=True,\n",
    ")\n",
    "\n",
    "print(f\"random seed equal to {rseed}\")\n",
    "# Generate data for testing\n",
    "m.generate_samples(1_000, random_seed=rseed)\n",
    "\n",
    "\n",
    "def sortAndRound(arr, n):\n",
    "    return [round(elem, n) for elem in np.sort(arr)]\n",
    "\n",
    "\n",
    "# print(\"Original probs:\",round_sort(m.probs, 4),'\\n')\n",
    "res = m.EM_sieving(\n",
    "    m.samples,\n",
    "    iter_initial=8,\n",
    "    num_candid=50,\n",
    "    num_best_candid=10,\n",
    "    accur_final=0.001,\n",
    "    random_seed=rseed,\n",
    ")\n",
    "\n",
    "print(\"Original mixture:\", sortAndRound(m.probs, 4), m.mus, m.sigmas, sep=\"\\n\")\n",
    "print(\"Sieved EM result:\", *res, sep=\"\\n\")\n",
    "orig_mat = np.array([m.probs, m.mus, m.sigmas])\n",
    "pred_mat = np.array([res[0], res[1], res[2]])\n",
    "print(\"\\nNorma for not sorded matrixes: \", np.linalg.norm(pred_mat - orig_mat))\n",
    "print(\n",
    "    \"\\nDistance between originals and predictions - \",\n",
    "    round(np.linalg.norm(np.sort(m.__getattribute__(\"probs\")) - np.sort(res[0])), 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Reconstruction a(t) and b(t) coefficients as a time function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "$\\textbf{Задача 6}$. Реконструировать коэффициенты $a(t)$ и $b(t)$, то есть построить их «точечные» оценки\n",
    "путем использования оценок распределений коэффициентов уравнения (1), полученных в резуль-\n",
    "тате решения Задач 1 – 2, для построения оценок самих коэффициентов. В качестве таких оценок\n",
    "берутся математическое ожидание и среднеквадратическое отклонение оцененного распределе-\n",
    "ния:\n",
    "$$\n",
    "a(t) ≈ a(t) = \\sum_{k=1}^{K}{p_k a_k}, \\quad \n",
    "b(t) ≈ b(t) = \\sum_{k=1}^{K}{p_k\\cdot(b^{2}_k + a^{2}_k) − a(t)^2}\n",
    "$$\n",
    "Здесь t – время (положение окна), параметры $a_k , b_k , p_k$ также зависят от положения окна."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## For dBX only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPNUM = 3\n",
    "\n",
    "STEP = 60\n",
    "SIZE = 4300\n",
    "\n",
    "TIME = dataframe[\"ydhm_id\"].values[1:]\n",
    "dbx = dataframe[\"dBX\"].values[1:]\n",
    "\n",
    "mix_dBX = DynamicMixture(\n",
    "    num_comps=COMPNUM,\n",
    "    distrib=tensorflow_probability.distributions.Normal,\n",
    "    time_span=TIME,\n",
    "    window_shape=(SIZE, STEP),\n",
    ")\n",
    "\n",
    "mix_dBX.predict_light(data=dbx)\n",
    "a, b = mix_dBX.reconstruct_process_coef()\n",
    "line(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import leastsq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = mix_dBX.process_coefs[\"a\"]\n",
    "N = 1000  # number of data points\n",
    "t = np.linspace(0, len(data), N)\n",
    "t = np.array(range(len(data)))\n",
    "# f = 1.15247 # Optional!! Advised not to use\n",
    "\n",
    "guess_mean = np.mean(data)\n",
    "guess_std = 1\n",
    "guess_phase = 0\n",
    "guess_freq = 0.01\n",
    "guess_amp = 3\n",
    "\n",
    "# we'll use this to plot our first estimate. This might already be good enough for you\n",
    "data_first_guess = guess_std * np.sin(t + guess_phase) + guess_mean\n",
    "\n",
    "\n",
    "# Define the function to optimize, in this case, we want to minimize the difference\n",
    "# between the actual data and our \"guessed\" parameters\n",
    "def optimize_func(x, data):\n",
    "    return x[0] * np.sin(x[1] * t + x[2]) + x[3] - data\n",
    "\n",
    "\n",
    "params_sin = leastsq(optimize_func, [guess_amp, guess_freq, guess_phase, guess_mean])[0]\n",
    "est_amp, est_freq, est_phase, est_mean = params_sin\n",
    "\n",
    "# recreate the fitted curve using the optimized parameters\n",
    "data_fit = est_amp * np.sin(est_freq * t + est_phase) + est_mean\n",
    "\n",
    "# recreate the fitted curve using the optimized parameters\n",
    "fine_t = t  # np.arange(0,max(t),0.1)\n",
    "data_fit0 = est_amp * np.sin(est_freq * fine_t + est_phase) + est_mean\n",
    "\n",
    "plt.plot(t, data, \".\")\n",
    "# plt.plot(t, data_first_guess, label='first guess')\n",
    "plt.plot(fine_t, data_fit, label=\"after fitting\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "line(dict(sin=data_fit, orig=data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "norm_hypot_pval = kstest(data_fit - data, cdf=\"norm\").pvalue\n",
    "histogram(data_fit - data, title=f\"p-value = {norm_hypot_pval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mix_dBX.process_coefs[\"a\"] - data_fit0\n",
    "guess_mean = np.mean(data)\n",
    "guess_std = 0.0\n",
    "guess_phase = 0\n",
    "guess_freq = 0.05\n",
    "guess_amp = 0.03\n",
    "\n",
    "# we'll use this to plot our first estimate. This might already be good enough for you\n",
    "# data_first_guess = guess_std*np.sin(t+guess_phase) + guess_mean\n",
    "\n",
    "\n",
    "# Define the function to optimize, in this case, we want to minimize the difference\n",
    "# between the actual data and our \"guessed\" parameters\n",
    "def optimize_func(x, data):\n",
    "    return x[0] * np.sin(x[1] * t + x[2]) + x[3] - data\n",
    "\n",
    "\n",
    "params_sin = leastsq(optimize_func, [guess_amp, guess_freq, guess_phase, guess_mean])[0]\n",
    "est_amp, est_freq, est_phase, est_mean = params_sin\n",
    "\n",
    "# recreate the fitted curve using the optimized parameters\n",
    "data_fit = est_amp * np.sin(est_freq * t + est_phase) + est_mean\n",
    "\n",
    "# recreate the fitted curve using the optimized parameters\n",
    "fine_t = t  # np.arange(0,max(t),0.1)\n",
    "data_fit = est_amp * np.sin(est_freq * fine_t + est_phase) + est_mean\n",
    "\n",
    "plt.plot(t, data, \".\")\n",
    "# plt.plot(t, data_first_guess, label='first guess')\n",
    "plt.plot(fine_t, data_fit, label=\"after fitting\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "line(dict(sin=data_fit + data_fit0, orig=data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "line(dict(sin=data_fit, orig=data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "norm_hypot_pval = kstest(data_fit - data, cdf=\"norm\").pvalue\n",
    "histogram(\n",
    "    data_fit - data,\n",
    "    title=f\"p-value = {norm_hypot_pval}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## For all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPNUM = 3\n",
    "\n",
    "STEP = 60\n",
    "SIZE = 4300\n",
    "\n",
    "TIME = dataframe[\"ydhm_id\"].values[1:]\n",
    "\n",
    "dbx_mixture = DynamicMixture(\n",
    "    num_comps=COMPNUM,\n",
    "    distrib=tensorflow_probability.distributions.Normal,\n",
    "    time_span=TIME,\n",
    "    window_shape=(SIZE, STEP),\n",
    ")\n",
    "\n",
    "dby_mixture = DynamicMixture(\n",
    "    num_comps=COMPNUM,\n",
    "    distrib=tensorflow_probability.distributions.Normal,\n",
    "    time_span=TIME,\n",
    "    window_shape=(SIZE, STEP),\n",
    ")\n",
    "\n",
    "dbz_mixture = DynamicMixture(\n",
    "    num_comps=COMPNUM,\n",
    "    distrib=tensorflow_probability.distributions.Normal,\n",
    "    time_span=TIME,\n",
    "    window_shape=(SIZE, STEP),\n",
    ")\n",
    "\n",
    "vx_mixture = DynamicMixture(\n",
    "    num_comps=COMPNUM,\n",
    "    distrib=tensorflow_probability.distributions.Normal,\n",
    "    time_span=TIME,\n",
    "    window_shape=(SIZE, STEP),\n",
    ")\n",
    "\n",
    "vy_mixture = DynamicMixture(\n",
    "    num_comps=COMPNUM,\n",
    "    distrib=tensorflow_probability.distributions.Normal,\n",
    "    time_span=TIME,\n",
    "    window_shape=(SIZE, STEP),\n",
    ")\n",
    "\n",
    "vz_mixture = DynamicMixture(\n",
    "    num_comps=COMPNUM,\n",
    "    distrib=tensorflow_probability.distributions.Normal,\n",
    "    time_span=TIME,\n",
    "    window_shape=(SIZE, STEP),\n",
    ")\n",
    "\n",
    "MIXTURES = [dbx_mixture, dby_mixture, dbz_mixture, vx_mixture, vy_mixture, vz_mixture]\n",
    "\n",
    "dbx = dataframe[\"dBX\"].values[1:]\n",
    "dby = dataframe[\"dBY\"].values[1:]\n",
    "dbz = dataframe[\"dBZ\"].values[1:]\n",
    "\n",
    "vx = dataframe[\"Vx_Velocity\"].values[1:]\n",
    "vy = dataframe[\"Vy_Velocity\"].values[1:]\n",
    "vz = dataframe[\"Vz_Velocity\"].values[1:]\n",
    "\n",
    "DATA = [dbx, dby, dbz, vx, vy, vz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvx = dataframe[\"dVX\"].values[1:]\n",
    "dvy = dataframe[\"dVY\"].values[1:]\n",
    "dvz = dataframe[\"dVZ\"].values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mixture, data in zip(MIXTURES, DATA):\n",
    "    # calculate parameters\n",
    "    mixture.predict_light(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "COEFS = []\n",
    "for mixt in MIXTURES:\n",
    "    a, b = mixt.reconstruct_process_coef()\n",
    "    coefs = dict(a=a, b=b)\n",
    "\n",
    "    COEFS.append(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Visualization: pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly as plt\n",
    "from plotly.express import line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = [\"dBX\", \"dBY\", \"dBZ\", \"VX\", \"VY\", \"VZ\"]\n",
    "\n",
    "for coefs, name in zip(COEFS, var_name):\n",
    "    fig = line(\n",
    "        coefs,\n",
    "        title=name + \" | dX(t) = a(t) dt + b(t) dW\\t\" + \"| Window length=4500, step=60\",\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "var_name = [\"dBX\", \"dBY\", \"dBZ\", \"VX\", \"VY\", \"VZ\"]\n",
    "for mix, name in zip(MIXTURES, var_name):\n",
    "    bold_name = f\"<b>{name}</b>\"\n",
    "    fig = mix.show_parameters()\n",
    "    fig.update_layout(\n",
    "        height=1000,\n",
    "        width=1200,\n",
    "        title=dict(\n",
    "            text=f\"{bold_name} {COMPNUM}-components mixture parameters. Window size is {SIZE} and step is {STEP}\",\n",
    "            x=0.5,\n",
    "            font=dict(size=22, color=\"#000000\"),\n",
    "        ),\n",
    "    )\n",
    "    file_name = f\"{name}_params\"\n",
    "    fig.write_html(join(\"Figures\", file_name) + \".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "RAM is filled with useless (at this point), but heavy objects. \n",
    "So it is wise to clean it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mix in MIXTURES:\n",
    "    del mix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "RAM is filled with useless (at this point), but heavy objects. \n",
    "So it is wise to clean it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mix in MIXTURES:\n",
    "    del mix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Draft: parameters ordering ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procpar(pk, ak, bk):\n",
    "    \"for a specific window\"\n",
    "    a = np.sum(pk * ak)\n",
    "    b = np.sum(pk * (bk**2 * ak**2) - a**2)\n",
    "    return a, b\n",
    "\n",
    "\n",
    "ar1 = np.array([1, 2, 3])\n",
    "ar2 = np.array([7, -1, 2])\n",
    "procpar(ar1, ar2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind(i, dic):\n",
    "    return dic[\"probs\"][i], dic[\"mus\"][i], dic[\"sigmas\"][i]\n",
    "\n",
    "\n",
    "res1 = get_ind(177, mixture.parameters)\n",
    "res2 = get_ind(178, mixture.parameters)\n",
    "res3 = get_ind(179, mixture.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res1, \"\\n\", res2, \"\\n\", res3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Assume we have a matrix of same-size lists, that contains integer numbers, that \n",
    "correspond to some order of mixture parameters.\n",
    "On each neighbouring windows we want to compare those matrixes to find out if\n",
    "any parameters variables crossed each other and change raletive positions. \n",
    "For this case of course basic ordering (for example by increasing of standard\n",
    "diviation) not working because this approach exclude oppartunity for \n",
    "this specific parameter to have intesetions between different components.\n",
    "\n",
    "It means that we need to think of something trickier.\n",
    "Let's try implenet next approach:\n",
    "1. Initialize some integer values to track order. For expamle - get indexes of\n",
    "increasing sorting of each array. And sort all values by one of the rows \n",
    "(for instance by sigmas). Thereupon save this values into matrix to ceep the track on.\n",
    "2. Calculate values on next window, sort by previously chosen row, \n",
    "save them in new matrix and compare them to previous 'order matrix'.\n",
    "3. If those matrixes are equal, then save current order matrix into previos and\n",
    "move on step 2.\n",
    "4. Otherwise at list one array differes at list by two values \n",
    "\n",
    "(example: \n",
    "$\\quad[[1,2,0][0,2,1]\\textbf{[0,1,2]}] vs [[1,2,0][0,2,1] \\textbf{[2,1,0]}] \\quad$\n",
    ", - first and last arguments differes). \n",
    "\n",
    "Chance of two arrays change their values at the same time is orbitrary low (I think).\n",
    "So if amended array is not on the same row that we are using for sorting, than \\\n",
    "we're fine. We've just tracked intersection. Move on step 2.\n",
    "5.  However if this array is on the same row that we are sorting by, then there \n",
    "must be applied additional actions. We need to take other row from 'previous \n",
    "order matrix'. See how it correspond to originaly chosen row, save their\n",
    "'relation' and resort current order matrix's original row. Other rows shouldn't\n",
    "be altered!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Also it is obvious \n",
    "that only columns may switch their places. It means them there is no use in sorting values by line\n",
    "I need to think of 'vertical' ordering. For ecamle I cand calculated initial orders and make dictionary of \n",
    "three elements: 'first line\", ..,'third line'. All this elements will contain a 1-D vector.\n",
    "And if the values changes than columns are switching. It means that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order(*args):\n",
    "    order_matrix = []\n",
    "    indexs = range(len(args[0]))\n",
    "\n",
    "    for arg in args:\n",
    "        order_matrix.append(sorted(indexs, key=lambda k: arg[k]))\n",
    "    return order_matrix\n",
    "\n",
    "\n",
    "def order_by(indexs, args):\n",
    "    for arg in args:\n",
    "        arg = np.array([arg[i] for i in indexs])\n",
    "        print(arg)\n",
    "\n",
    "\n",
    "def alter_order(ordmat_prev: list, ordmat_cur: list):\n",
    "    from copy import deepcopy\n",
    "\n",
    "    omp = deepcopy(ordmat_prev)\n",
    "    omc = deepcopy(ordmat_cur)\n",
    "    print(omp != omc)\n",
    "    if omp != omc:\n",
    "        print(omp, omc)\n",
    "        omp.pop(-1)\n",
    "        omc.pop(-1)\n",
    "        if omp != omc:\n",
    "            return omp.pop(-1)\n",
    "        else:\n",
    "            alter_order(omp, omc)\n",
    "    # return omc.pop(-1)\n",
    "    # if all([a_id == b_id for a_id, b_id in zip(omp, omc)]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapes_ones(ar1, ar2):\n",
    "    pairs = []\n",
    "    for el1, el2 in zip(ar1, ar2):\n",
    "        if (el1 == el2) or (any([el1 in pair for pair in pairs])):\n",
    "            continue\n",
    "        else:\n",
    "            pairs.append([el1, el2])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def swap(pairs, arr):\n",
    "    for i, ar in enumerate(arr):\n",
    "        for par in pairs:\n",
    "            if ar in par:\n",
    "                par.remove(ar)\n",
    "                print(int(par), ar)\n",
    "                arr[i] = par"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "# [Kolmogorov-Smironov test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Might be useful:\n",
    "* python [Kolmogorov-Smirnov one-sided test statistic distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ksone.html#scipy.stats.ksone)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## Static mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "First of all let's try this test on a basic data such as static mixture.\n",
    "For that reason will use `class mixture` and go through next steps:\n",
    "1. Generate samples for a mixture\n",
    "2. Shuffle all it's values and separete them into testing and validational\n",
    "sets\n",
    "3. Apply EM algorithm onto testing data to calculate mixture parameters and\n",
    "calcualte with `mixture.construct_tpf_mixture` [cumulative distribution function](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily#cdf) \n",
    "for a mixture \n",
    "4. Run K.S. test on validation data and cdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_mixture = DynamicMixture(\n",
    "    num_comps=3,\n",
    "    distrib=tensorflow_probability.distributions.Normal,\n",
    "    time_span=dataframe[\"ydhm_id\"].values,\n",
    "    window_shape=(4500, STEP),\n",
    ")\n",
    "\n",
    "vx = dataframe[\"Vx_Velocity\"].values\n",
    "vx_mixture.windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_mixture.rewrite_as_normal_human_this_initialization(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_mixture.predict_ks(vx, train_perc=0.5, relprev_pos=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "import tensorflow_probability\n",
    "\n",
    "\n",
    "## additional stuff; output appearancece\n",
    "def custom_print(text, *args, func=__Round):\n",
    "    print(text, f\"Orig - {func(args[0])}\", f\"Iter - {func(args[1])}\", sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 1 \"\"\"\n",
    "# Create mixture model\n",
    "m = Mixture(num_comps=3, distrib=tensorflow_probability.distributions.Normal)\n",
    "# np.random.seed()\n",
    "rseed = np.random.randint(1_000)\n",
    "m.initialize_probs_mus_sigmas(random_seed=rseed)\n",
    "print(f\"radnom seed equal to {rseed}\")\n",
    "# Generate data for testing\n",
    "m.generate_samples(1_000, random_seed=57)\n",
    "\n",
    "samples = m.samples.copy()\n",
    "\n",
    "\"\"\" 2 \"\"\"\n",
    "np.random.shuffle(samples)\n",
    "test_perc = 0.6\n",
    "test_size = int(test_perc * len(samples))\n",
    "\n",
    "test = samples[:test_size]\n",
    "valid = samples[test_size:]\n",
    "\n",
    "\"\"\" 3 \"\"\"\n",
    "pit, mit, sit, llh = m.EM_iterative(test, 30)\n",
    "tpf_m_on_test = mixture.construct_tpf_mixture(pit, mit, sit)\n",
    "# # EM comparison\n",
    "# pit, mit, sit, llhit = m.EM_iterative(samples, 30)\n",
    "\n",
    "\"\"\" 4 \"\"\"\n",
    "m_test = Mixture(\n",
    "    num_comps=m.num_comps,\n",
    "    distrib=m.distrib,\n",
    "    comp_probs=pit,\n",
    "    math_expects=mit,\n",
    "    stand_devs=sit,\n",
    ")\n",
    "\n",
    "\n",
    "def mixture_cdf(x):\n",
    "    return tpf_m_on_test.cdf(x).numpy()\n",
    "\n",
    "\n",
    "kolmog_test = kstest(valid, mixture_cdf)\n",
    "\n",
    "custom_print(\"Components probabilities:\", m.probs, pit)\n",
    "custom_print(\"Mathematical expectations:\", m.mus, mit)\n",
    "custom_print(\"Standard deviations:\", m.sigmas, sit)\n",
    "# custom_print(\"Log-likelihoods:\", m.log_likelihood(test_data1), llhit,\n",
    "#              func=_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pks, mks, sks, lks = m.Kolmogorov_EM(m.samples, relprev_pos=1, random_seed=80)\n",
    "custom_print(\"Components probabilities:\", m.probs, pks)\n",
    "custom_print(\"Mathematical expectations:\", m.mus, mks)\n",
    "custom_print(\"Standard deviations:\", m.sigmas, sks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.em.algorithms import KS_test\n",
    "\n",
    "KS_test(valid, probs=pit, mus=mit, sigmas=sit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "# Trigonometric approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"full_data.csv\")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "`Code to read from 1min_csv folder`\n",
    "<!-- \n",
    "path = 'Data_nice/1min_csv'\n",
    "files = os.listdir(path=path)\n",
    "files.remove('00readme.txt')\n",
    "files.sort()\n",
    "print(files)\n",
    "dfs = []\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer=os.path.join(path, files[0]),\n",
    "    na_values=['-1.00E+31','1.00E+31'],\n",
    "    converters={12: float, 13: float, 14:float, 15:float}\n",
    "    )\n",
    "dfs.append(df)\n",
    "for filename in files[1:]:\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=os.path.join(path, filename),\n",
    "        na_values=['-1.00E+31','1.0E+31', '-1e+31'],\n",
    "        converters={12: float, 13: float, 14:float, 15:float}\n",
    "    )\n",
    "    dfs.append(df)\n",
    "data = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "def tri(df):\n",
    "    # Создание столбца 'ydhm_id'\n",
    "        # Функция для преобразования значения к нужному виду\n",
    "    def format_value(value):\n",
    "        return f'{value:02}'\n",
    "    \n",
    "    make_cell = lambda row: str(\n",
    "        f\"{row['Year']:02}\"+\n",
    "        f\"-{row['Month']:02}\"+\n",
    "        f\"-{row['Day']:02}\"\n",
    "        f\"T{row['Hour']:02}\"+\n",
    "        f\":{row['Min']:02}\")\n",
    "    df['ydhm_id'] = df.apply(make_cell, axis=1) -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps = []\n",
    "for i, elem in enumerate((data[\"BX_GSE\"].values)):\n",
    "    if pd.isna(elem):\n",
    "        gaps.append(i)\n",
    "\n",
    "\n",
    "def omit_gaps(ar: list, dif=3):\n",
    "    s0 = ar[0]\n",
    "    newar = []\n",
    "    bad = []\n",
    "    for space in ar[1:]:\n",
    "        if space - s0 <= dif:\n",
    "            bad.append(space)\n",
    "            if abs(ar[-1] - space) <= dif:\n",
    "                ar.pop()\n",
    "                ar.append(f\"bad iters from {space}\")\n",
    "        else:\n",
    "            newar.append(space)\n",
    "        s0 = space\n",
    "    return newar, bad\n",
    "\n",
    "\n",
    "g1, b1 = omit_gaps(gaps, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "`Gaps visualization`\n",
    "<!-- fig = make_subplots(\n",
    "    rows=1, \n",
    "    cols=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=b1, \n",
    "        y=[1]*len(b1),\n",
    "        name='bad',\n",
    "        mode='markers'\n",
    "    ), \n",
    "    row=1, \n",
    "    col=1\n",
    "    )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=g1, \n",
    "        y=[1.1]*len(g1),\n",
    "        name='good',\n",
    "        mode='markers'\n",
    "    ), \n",
    "    row=1, \n",
    "    col=1\n",
    "    ) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 196853\n",
    "stop = 279681"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "bx = data[\"BX_GSE\"][start:stop].values\n",
    "# by = data['BY_GSE'][start:stop].values\n",
    "# bz = data['BZ_GSE'][start:stop].values\n",
    "time = data[\"ydhm_id\"][start:stop].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps(data, neighs=2):\n",
    "    for i, val in enumerate(data):\n",
    "        if pd.isna(val):\n",
    "            data[i] = np.mean(\n",
    "                bx[i - neighs : i + neighs][~np.isnan(bx[neighs - 2 : neighs + 2])]\n",
    "            )\n",
    "\n",
    "\n",
    "fill_gaps(bx)\n",
    "\n",
    "# fill_gaps(by)\n",
    "# fill_gaps(bz)\n",
    "# for i, val in enumerate(by):\n",
    "#     if pd.isna(val):\n",
    "#         print(i,\"Lnlj\")\n",
    "\n",
    "# line(dict(bx=bx, by=by, bz=bz), x=time, y=['bx', 'by', 'bz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increm(arr):\n",
    "    new_ar = [None]\n",
    "    for i in range(1, len(arr)):\n",
    "        inc = arr[i] - arr[i - 1]\n",
    "        new_ar.append(inc)\n",
    "    return new_ar\n",
    "\n",
    "\n",
    "def smooth(data, wind_size=20):\n",
    "    from numpy.lib.stride_tricks import sliding_window_view\n",
    "    from numpy import mean\n",
    "\n",
    "    windows = sliding_window_view(data, wind_size)\n",
    "    smoothed = []\n",
    "    for wind in windows:\n",
    "        smoothed.append(mean(wind))\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "dbx = increm(bx)[1:]\n",
    "\n",
    "# dby = increm(by)[1:]\n",
    "# dbz = increm(bz)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPNUM = 5\n",
    "\n",
    "STEP = 60\n",
    "SIZE = 4300\n",
    "TIME = time[int(SIZE / 2) : len(dbx) - int(SIZE / 2) : STEP]\n",
    "\n",
    "mix_dBX = DynamicMixture(\n",
    "    num_comps=COMPNUM,\n",
    "    distrib=tensorflow_probability.distributions.Normal,\n",
    "    time_span=TIME,\n",
    "    window_shape=(SIZE, STEP),\n",
    ")\n",
    "mix_dBX.predict_light(data=dbx)\n",
    "ax, bx = mix_dBX.reconstruct_process_coef()\n",
    "\n",
    "# mix_dBY = DynamicMixture(\n",
    "#     num_comps=COMPNUM,\n",
    "#     distrib=tensorflow_probability.distributions.Normal,\n",
    "#     time_span=TIME,\n",
    "#     window_shape=(SIZE, STEP)\n",
    "#     )\n",
    "# mix_dBY.predict_light(data=dby)\n",
    "# ay,by = mix_dBY.reconstruct_process_coef()\n",
    "\n",
    "# mix_dBZ = DynamicMixture(\n",
    "#     num_comps=COMPNUM,\n",
    "#     distrib=tensorflow_probability.distributions.Normal,\n",
    "#     time_span=TIME,\n",
    "#     window_shape=(SIZE, STEP)\n",
    "#     )\n",
    "# mix_dBZ.predict_light(data=dbz)\n",
    "# az,bz = mix_dBZ.reconstruct_process_coef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_approximation(data, time, harmonics_num=4, title=\"\"):\n",
    "    import numpy as np\n",
    "    from scipy.optimize import leastsq\n",
    "    import plotly.graph_objects as go\n",
    "    from scipy.stats import kstest, norm\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    def find_frequency(data, sampling_rate):\n",
    "        n = len(data)\n",
    "        # t = np.arange(0, n) / sampling_rate\n",
    "        fft_result = np.fft.fft(data)\n",
    "        freqs = np.fft.fftfreq(n, d=1 / sampling_rate)\n",
    "        spectrum = abs(fft_result)\n",
    "\n",
    "        idx = np.argmax(spectrum[1:]) + 1  # Избегаем нулевую частоту\n",
    "        freq = freqs[idx]\n",
    "\n",
    "        return abs(freq)\n",
    "\n",
    "    fig = make_subplots(rows=3, cols=1)\n",
    "    data_orig = data\n",
    "    t = np.array(range(len(data)))\n",
    "    params = []\n",
    "    harmonical_signal = np.zeros(len(data))\n",
    "\n",
    "    for _ in range(harmonics_num):\n",
    "        guess_mean = np.mean(data)\n",
    "        guess_phase = 0\n",
    "        guess_freq = find_frequency(data, len(data)) * np.pi * 2 / len(data)\n",
    "        guess_amp = max(data) - min(data)\n",
    "\n",
    "        def optimize_func(x):\n",
    "            return x[0] * np.sin(x[1] * t + x[2]) + x[3] - data\n",
    "\n",
    "        params_sin = leastsq(\n",
    "            optimize_func, [guess_amp, guess_freq, guess_phase, guess_mean]\n",
    "        )[0]\n",
    "\n",
    "        params.append(params_sin)\n",
    "        est_amp, est_freq, est_phase, est_mean = params_sin\n",
    "\n",
    "        data_fit = est_amp * np.sin(est_freq * t + est_phase) + est_mean\n",
    "        data = data - data_fit\n",
    "        harmonical_signal += data_fit\n",
    "\n",
    "    # Approximation visualization\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time, y=data_orig, marker=dict(color=\"#3058B0\")),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time, y=harmonical_signal, marker=dict(color=\"#FF7F50\")),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        xref=\"x domain\",\n",
    "        yref=\"y domain\",\n",
    "        x=0.5,\n",
    "        y=1.2,\n",
    "        showarrow=False,\n",
    "        font=dict(size=22),\n",
    "        text=\"<b>Approximation<b>\",\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # Residuals\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time, y=data, marker=dict(color=\"#FF7F50\")), row=2, col=1\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        xref=\"x domain\",\n",
    "        yref=\"y domain\",\n",
    "        x=0.5,\n",
    "        y=1.2,\n",
    "        showarrow=False,\n",
    "        font=dict(size=22),\n",
    "        text=\"<b>Residuals<b>\",\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # Histogramm\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=data, histnorm=\"probability density\", marker=dict(color=\"#3058B0\")\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "    x = np.linspace(min(data) * 1.2, max(data) * 1.2, 100)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x, y=norm.pdf(x, *norm.fit(data)), marker=dict(color=\"#FF7F50\")),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    def cdf(x):\n",
    "        return norm.cdf(x, loc=norm.fit(data)[0], scale=norm.fit(data)[1])\n",
    "\n",
    "    pval = kstest(data, cdf=cdf).pvalue\n",
    "    fig.add_annotation(\n",
    "        xref=\"x domain\",\n",
    "        yref=\"y domain\",\n",
    "        x=0.5,\n",
    "        y=1.3,\n",
    "        showarrow=False,\n",
    "        font=dict(size=22),\n",
    "        text=\"<b>Residuals histogram<b>\",\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        xref=\"x domain\",\n",
    "        yref=\"y domain\",\n",
    "        x=0.5,\n",
    "        y=1.2,\n",
    "        showarrow=False,\n",
    "        font=dict(size=20),\n",
    "        text=f\"Kolmogorov-Smirnov p-value = {pval:.3f}\",\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1500, height=1200, title=dict(font=dict(size=20), text=f\"<b>{title}<b>\")\n",
    "    )\n",
    "    return fig, data, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_test(resid, pvalue=0.05, smw=(50, 1)):\n",
    "    from numpy.lib.stride_tricks import sliding_window_view\n",
    "    from scipy.stats import kstest, norm\n",
    "\n",
    "    ws = sliding_window_view(resid, smw[0])[:: smw[1]]\n",
    "    P_vals = []\n",
    "    for frame in ws:\n",
    "\n",
    "        def cdf(x):\n",
    "            return norm.cdf(x, loc=norm.fit(frame)[0], scale=norm.fit(frame)[1])\n",
    "\n",
    "        pval = kstest(frame, cdf=cdf).pvalue\n",
    "        P_vals.append(pval)\n",
    "        if pval <= pvalue:\n",
    "            raise ValueError(\"No normal distribution on windows occure\")\n",
    "    return line(dict(pvalue=P_vals, significance_level=[pvalue] * len(P_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# line(dict(ax=ax, ay=ay, az=az), x=TIME, y=['ax','ay', 'az'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dbx_smoothed = smooth(mix_dBX.process_coefs[\"a\"], wind_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_x_smoothed, resid_x_smoothed, harmcs_x_smoothed = harmonic_approximation(\n",
    "    a_dbx_smoothed, TIME, title=\"dBX GSE\", harmonics_num=9\n",
    ")\n",
    "fig_x_smoothed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx(params_harms, data, freq_scale=1.0):\n",
    "    t = np.array(range(len(data)))\n",
    "    harmonical_signal = np.zeros(len(data))\n",
    "\n",
    "    for params_sin in params_harms:\n",
    "        est_amp, est_freq, est_phase, est_mean = params_sin\n",
    "        est_freq *= freq_scale\n",
    "        data_fit = est_amp * np.sin(est_freq * t + est_phase) + est_mean\n",
    "        data = data - data_fit\n",
    "        harmonical_signal += data_fit\n",
    "    return data, harmonical_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "d, h = approx(\n",
    "    harmcs_x_smoothed,\n",
    "    mix_dBX.process_coefs[\"a\"],\n",
    "    freq_scale=(len(resid_x_smoothed) / len(mix_dBX.process_coefs[\"a\"])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "line({\"data\": mix_dBX.process_coefs[\"a\"], \"harms\": h})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test(d, smw=(120, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_x, resid_x, harmcs_x = harmonic_approximation(\n",
    "    mix_dBX.process_coefs[\"a\"], TIME, title=\"dBX GSE\", harmonics_num=5\n",
    ")\n",
    "fig_x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "# Drafts in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(data, wind_size=20):\n",
    "    from numpy.lib.stride_tricks import sliding_window_view\n",
    "    from numpy import mean\n",
    "\n",
    "    windows = sliding_window_view(data, wind_size)\n",
    "    smoothed = []\n",
    "    for wind in windows:\n",
    "        smoothed.append(mean(wind))\n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{D}Z = \\mathbb{D}V + \\mathbb{E}U^2\n",
    "\\newline\n",
    "\\mathbb{E}U^2 = \\sum_{j=1}^{k}{\\sigma_j^2 \\cdot p_j} - \n",
    "    \\text{диффузионная компонента}\n",
    "\\newline\n",
    "\\mathbb{D}V = \\sum_{j=1}^{k}{(\\mu_j - \\bar{\\mu}) \\cdot p_j}, \\quad \\text{где} \n",
    "    \\quad \\bar{\\mu} = \\sum_{j=1}^{k}{\\mu_j p_j}, - \n",
    "    \\text{динамическая компонента}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4s4L7ADEfcTf",
    "fobxJD6AfkQw",
    "-W8ZeWOXElPL",
    "vbhgYiyCFCex"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
